import streamlit as st
import os
import requests
import json
from faster_whisper import WhisperModel
from datetime import datetime

# ì„¤ì •ê°’ì— ë”°ë¼ ëª¨ë¸ì„ ë¡œë“œ (ì¸ìê°’ì´ ë°”ë€Œë©´ ìºì‹œ ìƒˆë¡œê³ ì¹¨)
@st.cache_resource
def load_whisper_model(model_size, device, compute_type):
    return WhisperModel(model_size, device=device, compute_type=compute_type)

def meeting_page():
    st.header("ğŸ™ï¸ íšŒì˜/ìŒì„± ë¶„ì„")
    
    # í˜„ì¬ ì„¤ì • ìƒíƒœ í‘œì‹œ
    st.info(f"í˜„ì¬ ì—”ì§„: Whisper **{st.session_state.whisper_model}** ({st.session_state.whisper_device}) | LLM: **{st.session_state.ollama_model}**")

    uploaded_file = st.file_uploader("ë…¹ìŒ íŒŒì¼ ì—…ë¡œë“œ", type=["mp3", "wav", "m4a"])

    if uploaded_file is not None:
        os.makedirs("data", exist_ok=True)
        save_path = os.path.join("data", uploaded_file.name)
        
        with open(save_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        if st.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary"):
            # 1. ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸° (Zero-Config)
            w_model = st.session_state.whisper_model
            w_device = st.session_state.whisper_device
            w_compute = st.session_state.whisper_compute
            
            # 2. ëª¨ë¸ ë¡œë“œ
            model = load_whisper_model(w_model, w_device, w_compute)
            
            st.markdown("### ğŸ“ STT ë³€í™˜ ì¤‘...")
            progress = st.progress(0)
            
            try:
                segments, info = model.transcribe(save_path, beam_size=5)
                full_text = ""
                
                for i, segment in enumerate(segments):
                    full_text += segment.text + " "
                    # ì§„í–‰ë¥  ì‹œê°ì  í‘œì‹œ (ì •í™•í•˜ì§„ ì•ŠìŒ)
                    if i < 100: progress.progress(i + 1)
                
                progress.progress(100)
                st.success("í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ")

                # 3. LLM ìš”ì•½
                st.markdown("### ğŸ§  AI ìš”ì•½ ì¤‘...")
                summary_text = ""
                placeholder = st.empty()
                
                prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜:\n{full_text}"
                
                # ì„¤ì •ëœ Ollama URL ì‚¬ìš©
                ollama_url = st.session_state.ollama_url
                ollama_model = st.session_state.ollama_model
                
                try:
                    payload = {"model": ollama_model, "prompt": prompt, "stream": True}
                    with requests.post(f"{ollama_url}/api/generate", json=payload, stream=True) as res:
                        for line in res.iter_lines():
                            if line:
                                data = json.loads(line.decode("utf-8"))
                                if "response" in data:
                                    summary_text += data["response"]
                                    placeholder.markdown(summary_text + "â–Œ")
                    placeholder.markdown(summary_text)

                    # 4. ì €ì¥ ë° ì •ë¦¬
                    record = {
                        "date": datetime.now().strftime("%Y-%m-%d %H:%M"),
                        "filename": uploaded_file.name,
                        "full_text": full_text,
                        "summary": summary_text,
                        "model": f"{w_model} / {ollama_model}"
                    }
                    st.session_state.meeting_history.append(record)

                    # [PRD: Storage Efficient] ì›ë³¸ ì‚­ì œ ì˜µì…˜ í™•ì¸
                    if st.session_state.auto_delete:
                        os.remove(save_path)
                        st.toast("ìš©ëŸ‰ ì ˆì•½ì„ ìœ„í•´ ì›ë³¸ íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.", icon="ğŸ—‘ï¸")
                    
                except Exception as e:
                    st.error(f"Ollama ì—°ê²° ì‹¤íŒ¨: {e}")

            except Exception as e:
                st.error(f"Whisper ì˜¤ë¥˜: {e}")