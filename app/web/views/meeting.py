import streamlit as st
import os
import requests
import json
from faster_whisper import WhisperModel
from datetime import datetime

# ëª¨ë¸ ìºì‹± (ë§¤ë²ˆ ë¡œë”©í•˜ì§€ ì•Šë„ë¡ ì„¤ì •)
@st.cache_resource
def load_whisper_model():
    # N100 CPU í™˜ê²½ì„ ê³ ë ¤í•˜ì—¬ 'tiny' ë˜ëŠ” 'base' ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
    # ì„±ëŠ¥ì´ ë¶€ì¡±í•˜ë©´ 'tiny', ì¢€ ë” ì •í™•í•œ ê±¸ ì›í•˜ë©´ 'small'ë¡œ ë³€ê²½ ê°€ëŠ¥
    return WhisperModel("base", device="cpu", compute_type="int8")

def meeting_page():
    st.header("ğŸ™ï¸ íšŒì˜ ë…¹ìŒ ë¶„ì„")
    st.caption("ë…¹ìŒ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ AIê°€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ìš”ì•½í•´ì¤ë‹ˆë‹¤.")

    # 1. íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("ë…¹ìŒ íŒŒì¼ ì„ íƒ (mp3, wav, m4a)", type=["mp3", "wav", "m4a"])

    if uploaded_file is not None:
        # íŒŒì¼ ì €ì¥ (ì„ì‹œ)
        save_path = os.path.join("data", uploaded_file.name)
        os.makedirs("data", exist_ok=True)
        
        with open(save_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        st.success(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_file.name}")

        # 2. í…ìŠ¤íŠ¸ ë³€í™˜ (STT) ë²„íŠ¼
        if st.button("ğŸ“ í…ìŠ¤íŠ¸ ë³€í™˜ ë° ìš”ì•½ ì‹œì‘"):
            model = load_whisper_model()
            
            with st.spinner("ì—´ì‹¬íˆ ë°›ì•„ì ëŠ” ì¤‘ì…ë‹ˆë‹¤... (CPU ì„±ëŠ¥ì— ë”°ë¼ ì‹œê°„ ì†Œìš”)"):
                segments, info = model.transcribe(save_path, beam_size=5)
                
                full_text = ""
                progress_bar = st.progress(0)
                
                # ë³€í™˜ ê³¼ì • ì‹¤ì‹œê°„ í‘œì‹œ
                for i, segment in enumerate(segments):
                    full_text += segment.text + " "
                    # (ì§„í–‰ë¥ ì€ ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì‹œê°ì  íš¨ê³¼ë§Œ)
                    if i % 10 == 0:
                        progress_bar.progress(min(i, 100))
                
                progress_bar.progress(100)
            
            st.success("ë³€í™˜ ì™„ë£Œ!")
            
            # ê²°ê³¼ ë³´ì—¬ì£¼ê¸°
            with st.expander("ì›ë¬¸ ë³´ê¸° (Transcript)", expanded=False):
                st.text_area("ì „ì²´ ëŒ€í™” ë‚´ìš©", full_text, height=200)

            # 3. AI ìš”ì•½ ìš”ì²­ (Ollama)
            st.markdown("### ğŸ§  AI íšŒì˜ ìš”ì•½")
            summary_placeholder = st.empty()
            summary_text = ""

            prompt = f"""
            ì•„ë˜ íšŒì˜ ë‚´ìš©ì„ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ìš”ì•½í•´ì¤˜.
            ì¤‘ìš”í•œ ê²°ì • ì‚¬í•­ê³¼ í•  ì¼(Action Item)ì„ ë”°ë¡œ ì •ë¦¬í•´ì¤˜.
            
            [íšŒì˜ ë‚´ìš©]
            {full_text}
            """

            try:
                OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
                payload = {
                    "model": "gemma2:2b",
                    "prompt": prompt,
                    "stream": True
                }
                
                with requests.post(f"{OLLAMA_URL}/api/generate", json=payload, stream=True) as response:
                    for line in response.iter_lines():
                        if line:
                            data = json.loads(line.decode("utf-8"))
                            if "response" in data:
                                summary_text += data["response"]
                                summary_placeholder.markdown(summary_text + "â–Œ")
                
                summary_placeholder.markdown(summary_text)
                
            except Exception as e:
                st.error(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")